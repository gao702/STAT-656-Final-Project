---
title: "causal_analysis"
author: "Yaxin Fang"
date: "2024-11-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE}
rm(list = ls())

library('rstan')
library('bayesplot')
library('ggplot2')
library('BART')

bayesplot_theme_set(theme_default(base_size = 24, base_family = "sans"))
```


# prepare data
```{r}
# edited to add "escs" and "wealth" to the data
data_full <- readRDS("./data/cleaned/cleaned_data.Rds")
```


```{r}
# only consider AUS - for better balance between treatment/control groups
data_AUS <- data_full[data_full$country == 'AUS', ]
table(data_AUS$public_private)
head(data_AUS)
```


```{r}
data <- data_AUS[, c("gender", "computer", "internet", "math", "read", "science", "public_private", "family_class", "wealth", "escs", "stu_wgt")]

# one-hot encoding of binary variables
data$gender <- as.numeric(data$gender == 'female')
data$computer <- as.numeric(data$computer == "yes")
data$internet <- as.numeric(data$internet == "yes")
data$public_private <- as.numeric(data$public_private == "private")
data$family_class <- as.numeric((data$family_class == "middle class"))

# combine "computer" and "internet" into "tech_access"
data$tech_access <- data$computer * data$internet

# log transformation of the scores - will transorm back when making predictions
data$math_log <- log(data$math)
data$read_log <- log(data$read)
data$science_log <- log(data$science)

head(data)
```


```{r}
X <- data[, c(1, 8:10, 12)]
X[,"Offset"] <- 1
t <- data$public_private

treat_ind <- data$public_private==1
control_ind <- data$public_private==0

X_treat <- X[treat_ind, ]
X_control <- X[control_ind, ]

head(X_treat)
head(X_control)
```


```{r}
element_prod_row <- function(input_matrix, vector){
  row_num = nrow(input_matrix)
  col_num = ncol(input_matrix)
  output_matrix = matrix(nrow=row_num, ncol=col_num)
  for (i in 1:row_num){
    output_matrix[i, ] = input_matrix[i, ] * vector
  }
  output_matrix
}

element_prod_col <- function(input_matrix, vector){
  row_num = nrow(input_matrix)
  col_num = ncol(input_matrix)
  output_matrix = matrix(nrow=row_num, ncol=col_num)
  for (j in 1:col_num){
    output_matrix[, j] = input_matrix[, j] * vector
  }
  output_matrix
}
```


# initialize models
```{r}
# outcome model 1 - bayesian linear regression
outcome_normal_code = "
data {
int<lower=0> N;      // number of data items
int<lower=0> N_test;   // number of test data items
int<lower=0> K;       // number of predictors
real<lower=0> pr_sd; // std dev of the prior
matrix[N, K] x;       // predictor matrix
real y[N];          // output vector
matrix[N_test, K] x_test;   // predictor matrix
}

parameters {
  vector[K] beta;       // coefficients for predictors
  real<lower=0> sigma2;  // error scale
}

transformed parameters {
  vector[N] mu = x * beta;
  real<lower=0> sigma = sqrt(sigma2);
}

model {
  beta ~ normal(0, pr_sd);  // Note: beta is k-dim
  sigma2 ~ inv_gamma(0.001,0.001);  // Close to a flat prior
  y ~ normal(mu,sigma);  // likelihood
}

generated quantities {
  real y_rep[N_test];
  y_rep = normal_rng(x_test * beta, sigma);
} "

outcome_normal = stan_model(model_code=outcome_normal_code)
```



```{r}
# propensity score model
ps_logis_code = "
data {
  int<lower=0> N;      // number of data items
  int<lower=0> K;      // number of predictors
  real<lower=0> pr_sd; // std dev of the prior
  matrix[N, K] x;      // predictor matrix
  int<lower=0, upper=1> y[N];      // Binary outcome variable
}

parameters {
  vector[K] beta;                  // Regression coefficients
}

transformed parameters {
  vector[N] mu = x * beta;
}

model {
  // Prior for beta
  beta ~ normal(0, pr_sd);           
  
  // Likelihood
  y ~ bernoulli_logit(mu);   // Logistic regression likelihood
}

generated quantities {
  int<lower=0, upper=1> y_rep[N];
  y_rep = bernoulli_logit_rng(mu);
} "

ps_logis = stan_model(model_code=ps_logis_code)
```


# causal estimate
```{r}
dr_estimate <- function(y, t, y0_pred, y1_pred, ps){
  y_pred = element_prod_row(y0_pred, 1-t) + element_prod_row(y1_pred, t)
  ps_pred = element_prod_row(1-ps, 1-t) + element_prod_row(ps, t) 
  temp1 = y1_pred - y0_pred
  temp2 = y - y_pred
  dr_estimate = temp1 + temp2/ps_pred
  
  dr_estimate
}
```


# propensity score model
```{r}
ps_data <- list(N = nrow(X), K = ncol(X), pr_sd = 3, x=X, y=t)
ps_fit <- sampling(ps_logis, data=ps_data, iter=5000, warmup=4000, chains=1)
```


```{r}
ps <- 1/(1 + exp(-extract(ps_fit)$mu))
```


```{r}
# check mixing 
class(ps_fit)
post_smp <- as.data.frame(ps_fit)
colnames(post_smp)[1:5] <- colnames(X)
#hist(post_smp$TV,50)
mcmc_areas(post_smp[,1:5], pars = colnames(X)[1:5], prob = 0.8)

plot(post_smp$gender, type='l')
plot(post_smp$family_class, type='l')
plot(post_smp$tech_access, type='l')
plot(post_smp$wealth, type='l')
plot(post_smp$escs, type='l')
```


```{r}
# predictive check on the propensity score model
y_rep = extract(ps_fit)$y_rep
# One posterior predictive check compares the number of 0's and 1's in the observed vs predictive datasets
ppc_bars(y=t,yrep=y_rep)+ggplot2::labs(y = "Number of 0's and 1's in observed and Predictive datasets")
# Another just plots the proportions of 1's
ppc_stat(y=t,yrep=y_rep)+ggplot2::labs(x = "Proportion of 1's in observed and Predictive datasets")
```


# outcome models

## math
```{r}
y <- data$math_log
y_treat <- y[treat_ind]
y_control <- y[control_ind]
```


### bayesian linear regression
```{r}
# treatment group: fit model
math_outcome_treat <- list(N = nrow(X_treat),
                           N_test = nrow(X),
                           K = ncol(X_treat),
                           pr_sd = 3,
                           x = X_treat,
                           y = y_treat,
                           x_test = X)
math_outcome_treat_BLR <- sampling(outcome_normal, data=math_outcome_treat, iter=5000, warmup=4000, chains=1)
```


```{r}
# treatment group: check mixing
class(math_outcome_treat_BLR)
post_smp <- as.data.frame(math_outcome_treat_BLR)
colnames(post_smp)[1:5] <- colnames(X)
 
mcmc_areas(post_smp[,1:5], pars = colnames(X)[1:5], prob = 0.8)
 
plot(post_smp$gender, type='l')
plot(post_smp$family_class, type='l')
plot(post_smp$tech_access, type='l')
```


```{r}
# treatment group: predictive check
y1_pred <- extract(math_outcome_treat_BLR)$y_rep
 
ppd_intervals(y1_pred[ ,treat_ind], x=y[treat_ind]) + geom_abline(intercept = 0, slope = 1)  +
   ggplot2::labs(y = "Predicted y's", x = "Observed y's")
 
ppd_intervals(t(t(y1_pred[ ,treat_ind])-y[treat_ind]),x=y[treat_ind]) + geom_abline(intercept = 0, slope = 0)  +
   ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's")
```

```{r}
# control group: fit model
math_outcome_control <- list(N = nrow(X_control),
                           N_test = nrow(X),
                           K = ncol(X_control),
                           pr_sd = 3,
                           x = X_control,
                           y = y_control,
                           x_test = X)
math_outcome_control_BLR <- sampling(outcome_normal, data=math_outcome_control, iter=5000, warmup=4000, chains=1)
```

```{r}
# control group: check mixing
class(math_outcome_control_BLR)
post_smp <- as.data.frame(math_outcome_control_BLR)
colnames(post_smp)[1:5] <- colnames(X)
 
mcmc_areas(post_smp[,1:5], pars = colnames(X)[1:5], prob = 0.8)
 
plot(post_smp$gender, type='l')
plot(post_smp$family_class, type='l')
plot(post_smp$tech_access, type='l')
```

```{r}
# control group: predictive check
y0_pred <- extract(math_outcome_control_BLR)$y_rep
 
ppd_intervals(y0_pred[ ,control_ind], x=y[control_ind]) + geom_abline(intercept = 0, slope = 1)  +
   ggplot2::labs(y = "Predicted y's", x = "Observed y's")
 
ppd_intervals(t(t(y0_pred[ ,control_ind])-y[control_ind]),x=y[control_ind]) + geom_abline(intercept = 0, slope = 0)  +
   ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's")
```


### BART

```{r}
# treatment group: fit model
math_outcome_treat_BART <- wbart(x.train=as.matrix(X_treat[1:5]), 
                            y.train = y_treat, 
                            x.test=as.matrix(X[1:5]),
                            w=data_AUS$stu_wgt)
```


```{r}
# treatment group: predictive check
y1_pred <- math_outcome_treat_BART$yhat.test

ppd_intervals(y1_pred[ ,treat_ind],x=y[treat_ind]) + geom_abline(intercept = 0, slope = 1)  +
  ggplot2::labs(y = "Predicted y's", x = "Observed y's", title="Math: Predictive check for treatment group")

ppd_intervals(t(t(y1_pred[ ,treat_ind])-y[treat_ind]),x=y[treat_ind]) + geom_abline(intercept = 0, slope = 0)  +
  ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's", title="Math: Predictive check for control group")
```


```{r}
# control group: fit model
math_outcome_control_BART <- wbart(x.train=as.matrix(X_control[1:5]), 
                              y.train = y_control, 
                              x.test=as.matrix(X[1:5]), 
                              w=data_AUS$stu_wgt)
```


```{r}
# control group: predictive check
y0_pred <- math_outcome_control_BART$yhat.test

ppd_intervals(y0_pred[ ,control_ind],x=y[control_ind]) + geom_abline(intercept = 0, slope = 1)  +
  ggplot2::labs(y = "Predicted y's", x = "Observed y's", title="Math: Predictive check for control group")

ppd_intervals(t(t(y0_pred[ ,control_ind])-y[control_ind]), x=y[control_ind]) + geom_abline(intercept = 0, slope = 0)  +
  ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's", title = "Math: Predictive check for control group")
```

## read
```{r}
y <- data$read_log
y_treat <- y[treat_ind]
y_control <- y[control_ind]
```

### bayesian linear regression
```{r}
# treatment group: fit model
read_outcome_treat <- list(N = nrow(X_treat),
                           N_test = nrow(X),
                           K = ncol(X_treat),
                           pr_sd = 3,
                           x = X_treat,
                           y = y_treat,
                           x_test = X)
read_outcome_treat_BLR <- sampling(outcome_normal, data=read_outcome_treat, iter=5000, warmup=4000, chains=1)
```


```{r}
# treatment group: check mixing
class(read_outcome_treat_BLR)
post_smp <- as.data.frame(read_outcome_treat_BLR)
colnames(post_smp)[1:5] <- colnames(X)
 
mcmc_areas(post_smp[,1:5], pars = colnames(X)[1:5], prob = 0.8)
 
plot(post_smp$gender, type='l')
plot(post_smp$family_class, type='l')
plot(post_smp$tech_access, type='l')
```


```{r}
# treatment group: predictive check
y1_pred <- extract(read_outcome_treat_BLR)$y_rep
 
ppd_intervals(y1_pred[ ,treat_ind], x=y[treat_ind]) + geom_abline(intercept = 0, slope = 1)  +
   ggplot2::labs(y = "Predicted y's", x = "Observed y's")
 
ppd_intervals(t(t(y1_pred[ ,treat_ind])-y[treat_ind]),x=y[treat_ind]) + geom_abline(intercept = 0, slope = 0)  +
   ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's")
```

```{r}
# control group: fit model
read_outcome_control <- list(N = nrow(X_control),
                           N_test = nrow(X),
                           K = ncol(X_control),
                           pr_sd = 3,
                           x = X_control,
                           y = y_control,
                           x_test = X)
read_outcome_control_BLR <- sampling(outcome_normal, data=read_outcome_control, iter=5000, warmup=4000, chains=1)
```

```{r}
# control group: check mixing
class(read_outcome_control_BLR)
post_smp <- as.data.frame(read_outcome_control_BLR)
colnames(post_smp)[1:5] <- colnames(X)
 
mcmc_areas(post_smp[,1:5], pars = colnames(X)[1:5], prob = 0.8)
 
plot(post_smp$gender, type='l')
plot(post_smp$family_class, type='l')
plot(post_smp$tech_access, type='l')
```

```{r}
# control group: predictive check
y0_pred <- extract(read_outcome_control_BLR)$y_rep
 
ppd_intervals(y0_pred[ ,control_ind], x=y[control_ind]) + geom_abline(intercept = 0, slope = 1)  +
   ggplot2::labs(y = "Predicted y's", x = "Observed y's")
 
ppd_intervals(t(t(y0_pred[ ,control_ind])-y[control_ind]),x=y[control_ind]) + geom_abline(intercept = 0, slope = 0)  +
   ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's")
```


### BART
```{r}
# treatment group: fit model
read_outcome_treat_BART <- wbart(x.train=as.matrix(X_treat[1:5]), 
                            y.train = y_treat, 
                            x.test=as.matrix(X[1:5]),
                            w=data_AUS$stu_wgt)
```


```{r}
# treatment group: predictive check
y1_pred <- read_outcome_treat_BART$yhat.test

ppd_intervals(y1_pred[ ,treat_ind],x=y[treat_ind]) + geom_abline(intercept = 0, slope = 1)  +
  ggplot2::labs(y = "Predicted y's", x = "Observed y's", title="Read: Predictive check for treatment group")

ppd_intervals(t(t(y1_pred[ ,treat_ind])-y[treat_ind]),x=y[treat_ind]) + geom_abline(intercept = 0, slope = 0)  +
  ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's", title="Read: Predictive check for control group")
```


```{r}
# control group: fit model
read_outcome_control_BART <- wbart(x.train=as.matrix(X_control[1:5]), 
                              y.train = y_control, 
                              x.test=as.matrix(X[1:5]), 
                              w=data_AUS$stu_wgt)
```


```{r}
# control group: predictive check
y0_pred <- read_outcome_control_BART$yhat.test

ppd_intervals(y0_pred[ ,control_ind],x=y[control_ind]) + geom_abline(intercept = 0, slope = 1)  +
  ggplot2::labs(y = "Predicted y's", x = "Observed y's", title="Read: Predictive check for control group")

ppd_intervals(t(t(y0_pred[ ,control_ind])-y[control_ind]), x=y[control_ind]) + geom_abline(intercept = 0, slope = 0)  +
  ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's", title = "Read: Predictive check for control group")
```


## science
```{r}
y <- data$science_log
y_treat <- y[treat_ind]
y_control <- y[control_ind]
```

### bayesian linear regression
```{r}
# treatment group: fit model
sci_outcome_treat <- list(N = nrow(X_treat),
                           N_test = nrow(X),
                           K = ncol(X_treat),
                           pr_sd = 3,
                           x = X_treat,
                           y = y_treat,
                           x_test = X)
sci_outcome_treat_BLR <- sampling(outcome_normal, data=sci_outcome_treat, iter=5000, warmup=4000, chains=1)
```


```{r}
# treatment group: check mixing
class(sci_outcome_treat_BLR)
post_smp <- as.data.frame(sci_outcome_treat_BLR)
colnames(post_smp)[1:5] <- colnames(X)
 
mcmc_areas(post_smp[,1:5], pars = colnames(X)[1:5], prob = 0.8)
 
plot(post_smp$gender, type='l')
plot(post_smp$family_class, type='l')
plot(post_smp$tech_access, type='l')
```


```{r}
# treatment group: predictive check
y1_pred <- extract(sci_outcome_treat_BLR)$y_rep
 
ppd_intervals(y1_pred[ ,treat_ind], x=y[treat_ind]) + geom_abline(intercept = 0, slope = 1)  +
   ggplot2::labs(y = "Predicted y's", x = "Observed y's")
 
ppd_intervals(t(t(y1_pred[ ,treat_ind])-y[treat_ind]),x=y[treat_ind]) + geom_abline(intercept = 0, slope = 0)  +
   ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's")
```

```{r}
# control group: fit model
sci_outcome_control <- list(N = nrow(X_control),
                           N_test = nrow(X),
                           K = ncol(X_control),
                           pr_sd = 3,
                           x = X_control,
                           y = y_control,
                           x_test = X)
sci_outcome_control_BLR <- sampling(outcome_normal, data=sci_outcome_control, iter=5000, warmup=4000, chains=1)
```

```{r}
# control group: check mixing
class(sci_outcome_control_BLR)
post_smp <- as.data.frame(sci_outcome_control_BLR)
colnames(post_smp)[1:5] <- colnames(X)
 
mcmc_areas(post_smp[,1:5], pars = colnames(X)[1:5], prob = 0.8)
 
plot(post_smp$gender, type='l')
plot(post_smp$family_class, type='l')
plot(post_smp$tech_access, type='l')
```

```{r}
# control group: predictive check
y0_pred <- extract(sci_outcome_control_BLR)$y_rep
 
ppd_intervals(y0_pred[ ,control_ind], x=y[control_ind]) + geom_abline(intercept = 0, slope = 1)  +
   ggplot2::labs(y = "Predicted y's", x = "Observed y's")
 
ppd_intervals(t(t(y0_pred[ ,control_ind])-y[control_ind]),x=y[control_ind]) + geom_abline(intercept = 0, slope = 0)  +
   ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's")
```


### BART
```{r}
# treatment group: fit the model
sci_outcome_treat_BART <- wbart(x.train=as.matrix(X_treat[1:5]), 
                               y.train = y_treat,
                               x.test=as.matrix(X[1:5]),
                               w=data_AUS$stu_wgt)
```


```{r}
# treatment group: predictive check
y1_pred <- sci_outcome_treat_BART$yhat.test

ppd_intervals(y1_pred[ ,treat_ind],x=y[treat_ind]) + geom_abline(intercept = 0, slope = 1)  +
  ggplot2::labs(y = "Predicted y's", x = "Observed y's", title="Read: Predictive check for treatment group")

ppd_intervals(t(t(y1_pred[ ,treat_ind])-y[treat_ind]),x=y[treat_ind]) + geom_abline(intercept = 0, slope = 0)  +
  ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's", title="Read: Predictive check for control group")
```


```{r}
# control group: fit the model
sci_outcome_control_BART <- wbart(x.train=as.matrix(X_control[1:5]), 
                                 y.train = y_control, 
                                 x.test=as.matrix(X[1:5]), 
                                 w=data_AUS$stu_wgt)
```


```{r}
# control group: predictive check
y0_pred <- sci_outcome_control_BART$yhat.test

ppd_intervals(y0_pred[ ,control_ind],x=y[control_ind]) + geom_abline(intercept = 0, slope = 1)  +
  ggplot2::labs(y = "Predicted y's", x = "Observed y's", title="Read: Predictive check for control group")

ppd_intervals(t(t(y0_pred[ ,control_ind])-y[control_ind]), x=y[control_ind]) + geom_abline(intercept = 0, slope = 0)  +
  ggplot2::labs(y = "Errors in predicted y's", x = "Observed y's", title = "Read: Predictive check for control group")
```


# calculate CATE - doubly-robust estimator

```{r}
group1_ind <- (X$gender == 1) & (X$family_class == 1) & (X$tech_access ==1)
group2_ind <- (X$gender == 0) & (X$family_class == 1) & (X$tech_access ==1)
group3_ind <- (X$gender == 1) & (X$family_class == 0) & (X$tech_access ==1)
group4_ind <- (X$gender == 0) & (X$family_class == 0) & (X$tech_access ==1)
group5_ind <- (X$gender == 1) & (X$family_class == 1) & (X$tech_access ==0)
group6_ind <- (X$gender == 0) & (X$family_class == 1) & (X$tech_access ==0)
group7_ind <- (X$gender == 1) & (X$family_class == 0) & (X$tech_access ==0)
group8_ind <- (X$gender == 0) & (X$family_class == 0) & (X$tech_access ==0)
```


## math
```{r}
y <- data$math
```

### bayesian linear regression
```{r}
y0_pred <- exp(extract(math_outcome_control_BLR)$y_rep)
y1_pred <- exp(extract(math_outcome_treat_BLR)$y_rep)

causal_contrasts <- dr_estimate(y, t, y0_pred, y1_pred, ps)

cate1_dr <- rowMeans(causal_contrasts[, group1_ind])
cate2_dr <- rowMeans(causal_contrasts[, group2_ind])
cate3_dr <- rowMeans(causal_contrasts[, group3_ind])
cate4_dr <- rowMeans(causal_contrasts[, group4_ind])
cate5_dr <- rowMeans(causal_contrasts[, group5_ind])
cate6_dr <- rowMeans(causal_contrasts[, group6_ind])
cate7_dr <- rowMeans(causal_contrasts[, group7_ind])
cate8_dr <- rowMeans(causal_contrasts[, group8_ind])
```

```{r}
cate_math_dr_BLR <- data.frame(cate1_dr, cate2_dr, cate3_dr, cate4_dr, cate5_dr, cate6_dr, cate7_dr, cate8_dr)
colnames(cate_math_dr_BLR) <-  c("female, middle, tech", 
                          "male, middle, tech", 
                          "female, working, tech", 
                          "male, working, tech", 
                          "female, middle, no_tech", 
                          "male, middle, no_tech", 
                          "female, working, no_tech", 
                          "male, working, no_tech")
par(mar = c(4, 11, 4, 2))
boxplot(cate_math_dr_BLR, horizontal=TRUE, las=1, main="Math: CATE (with BLR)")
```


### BART
```{r}
y0_pred <- exp(math_outcome_control_BART$yhat.test)
y1_pred <- exp(math_outcome_treat_BART$yhat.test)

causal_contrasts <- dr_estimate(y, t, y0_pred, y1_pred, ps)

cate1_dr <- rowMeans(causal_contrasts[, group1_ind])
cate2_dr <- rowMeans(causal_contrasts[, group2_ind])
cate3_dr <- rowMeans(causal_contrasts[, group3_ind])
cate4_dr <- rowMeans(causal_contrasts[, group4_ind])
cate5_dr <- rowMeans(causal_contrasts[, group5_ind])
cate6_dr <- rowMeans(causal_contrasts[, group6_ind])
cate7_dr <- rowMeans(causal_contrasts[, group7_ind])
cate8_dr <- rowMeans(causal_contrasts[, group8_ind])
```

```{r}
cate_math_dr_BART <- data.frame(cate1_dr, cate2_dr, cate3_dr, cate4_dr, cate5_dr, cate6_dr, cate7_dr, cate8_dr)
colnames(cate_math_dr_BART) <-  c("female, middle, tech", 
                          "male, middle, tech", 
                          "female, working, tech", 
                          "male, working, tech", 
                          "female, middle, no_tech", 
                          "male, middle, no_tech", 
                          "female, working, no_tech", 
                          "male, working, no_tech")
par(mar = c(4, 11, 4, 2))
boxplot(cate_math_dr_BART, horizontal=TRUE, las=1, main="Math: CATE (with BART)")


#boxplot(cate, las=2)
```

## read

```{r}
y <- data$read
```

### bayesian linear regression
```{r}
y0_pred <- exp(extract(read_outcome_control_BLR)$y_rep)
y1_pred <- exp(extract(read_outcome_treat_BLR)$y_rep)

causal_contrasts <- dr_estimate(y, t, y0_pred, y1_pred, ps)

cate1_dr <- rowMeans(causal_contrasts[, group1_ind])
cate2_dr <- rowMeans(causal_contrasts[, group2_ind])
cate3_dr <- rowMeans(causal_contrasts[, group3_ind])
cate4_dr <- rowMeans(causal_contrasts[, group4_ind])
cate5_dr <- rowMeans(causal_contrasts[, group5_ind])
cate6_dr <- rowMeans(causal_contrasts[, group6_ind])
cate7_dr <- rowMeans(causal_contrasts[, group7_ind])
cate8_dr <- rowMeans(causal_contrasts[, group8_ind])
```

```{r}
cate_read_dr_BLR <- data.frame(cate1_dr, cate2_dr, cate3_dr, cate4_dr, cate5_dr, cate6_dr, cate7_dr, cate8_dr)
colnames(cate_read_dr_BLR) <-  c("female, middle, tech", 
                          "male, middle, tech", 
                          "female, working, tech", 
                          "male, working, tech", 
                          "female, middle, no_tech", 
                          "male, middle, no_tech", 
                          "female, working, no_tech", 
                          "male, working, no_tech")
par(mar = c(4, 11, 4, 2))
boxplot(cate_read_dr_BLR, horizontal=TRUE, las=1, main="Read: CATE (with BLR)")
```

### BART
```{r}
y0_pred <- exp(read_outcome_control_BART$yhat.test)
y1_pred <- exp(read_outcome_treat_BART$yhat.test)

causal_contrasts <- dr_estimate(y, t, y0_pred, y1_pred, ps)

cate1_dr <- rowMeans(causal_contrasts[, group1_ind])
cate2_dr <- rowMeans(causal_contrasts[, group2_ind])
cate3_dr <- rowMeans(causal_contrasts[, group3_ind])
cate4_dr <- rowMeans(causal_contrasts[, group4_ind])
cate5_dr <- rowMeans(causal_contrasts[, group5_ind])
cate6_dr <- rowMeans(causal_contrasts[, group6_ind])
cate7_dr <- rowMeans(causal_contrasts[, group7_ind])
cate8_dr <- rowMeans(causal_contrasts[, group8_ind])
```


```{r}
cate_read_dr_BART <- data.frame(cate1_dr, cate2_dr, cate3_dr, cate4_dr, cate5_dr, cate6_dr, cate7_dr, cate8_dr)
colnames(cate_read_dr_BART) <- c("female, middle, tech", 
                          "male, middle, tech", 
                          "female, working, tech", 
                          "male, working, tech", 
                          "female, middle, no_tech", 
                          "male, middle, no_tech", 
                          "female, working, no_tech", 
                          "male, working, no_tech")
par(mar = c(4, 11, 4, 2))
boxplot(cate_read_dr_BART, horizontal=TRUE, las=1, main="Read: CATE (with BART)")


#boxplot(cate, las=2)
```


## science

```{r}
y <- data$science
```

### Bayesian Linear Regression
```{r}
y0_pred <- exp(extract(sci_outcome_control_BLR)$y_rep)
y1_pred <- exp(extract(sci_outcome_treat_BLR)$y_rep)

causal_contrasts <- dr_estimate(y, t, y0_pred, y1_pred, ps)

cate1_dr <- rowMeans(causal_contrasts[, group1_ind])
cate2_dr <- rowMeans(causal_contrasts[, group2_ind])
cate3_dr <- rowMeans(causal_contrasts[, group3_ind])
cate4_dr <- rowMeans(causal_contrasts[, group4_ind])
cate5_dr <- rowMeans(causal_contrasts[, group5_ind])
cate6_dr <- rowMeans(causal_contrasts[, group6_ind])
cate7_dr <- rowMeans(causal_contrasts[, group7_ind])
cate8_dr <- rowMeans(causal_contrasts[, group8_ind])
```

```{r}
cate_sci_dr_BLR <- data.frame(cate1_dr, cate2_dr, cate3_dr, cate4_dr, cate5_dr, cate6_dr, cate7_dr, cate8_dr)
colnames(cate_read_dr_BLR) <-  c("female, middle, tech", 
                          "male, middle, tech", 
                          "female, working, tech", 
                          "male, working, tech", 
                          "female, middle, no_tech", 
                          "male, middle, no_tech", 
                          "female, working, no_tech", 
                          "male, working, no_tech")
par(mar = c(4, 11, 4, 2))
boxplot(cate_sci_dr_BLR, horizontal=TRUE, las=1, main="Science: CATE (with BLR)")
```

### BART
```{r}
y0_pred <- exp(sci_outcome_control_BART$yhat.test)
y1_pred <- exp(sci_outcome_treat_BART$yhat.test)

causal_contrasts <- dr_estimate(y, t, y0_pred, y1_pred, ps)

cate1_dr <- rowMeans(causal_contrasts[, group1_ind])
cate2_dr <- rowMeans(causal_contrasts[, group2_ind])
cate3_dr <- rowMeans(causal_contrasts[, group3_ind])
cate4_dr <- rowMeans(causal_contrasts[, group4_ind])
cate5_dr <- rowMeans(causal_contrasts[, group5_ind])
cate6_dr <- rowMeans(causal_contrasts[, group6_ind])
cate7_dr <- rowMeans(causal_contrasts[, group7_ind])
cate8_dr <- rowMeans(causal_contrasts[, group8_ind])
```


```{r}
cate_science_dr_BART <- data.frame(cate1_dr, cate2_dr, cate3_dr, cate4_dr, cate5_dr, cate6_dr, cate7_dr, cate8_dr)
colnames(cate_science_dr_BART) <- c("female, middle, tech", 
                               "male, middle, tech", 
                               "female, working, tech", 
                               "male, working, tech", 
                               "female, middle, no_tech", 
                               "male, middle, no_tech", 
                               "female, working, no_tech", 
                               "male, working, no_tech")
par(mar = c(4, 11, 4, 2))
boxplot(cate_science_dr_BART, horizontal=TRUE, las=1, main="Science: CATE (with BART)")


#boxplot(cate, las=2)
```


# some visualizations

```{r}
hist(X_treat$wealth, col = rgb(0, 0, 1, 0.5), xlim = c(-7, 7), ylim=c(0, 1600),
     main = "wealth", xlab = "wealth", breaks = 30)
hist(X_control$wealth, col = rgb(1, 0, 0, 0.5), add = TRUE, breaks = 30)

legend("topright", legend = c("private", "public"), 
       fill = c(rgb(0, 0, 1, 0.5), rgb(1, 0, 0, 0.5)))
```

```{r}
hist(X_treat$wealth, col = rgb(0, 0, 1, 0.5), xlim = c(-7, 7), ylim=c(0, 1600),
     main = "wealth", xlab = "wealth", breaks = 30)
hist(X_control$wealth, col = rgb(1, 0, 0, 0.5), add = TRUE, breaks = 30)

legend("topright", legend = c("private", "public"), 
       fill = c(rgb(0, 0, 1, 0.5), rgb(1, 0, 0, 0.5)))
```


```{r}
hist(X_treat$escs, col = rgb(0.2, 0.8, 0.2, 0.5), xlim = c(-7, 7), ylim=c(0, 700),
     main = "escs", xlab = "escs", breaks = 30)
hist(X_control$escs, col = rgb(1, 0.6, 0, 0.5), add = TRUE, breaks = 30)

legend("topright", legend = c("private", "public"), 
       fill = c(rgb(0.2, 0.8, 0.2, 0.5), rgb(1, 0.6, 0, 0.5)))
```

```{r}
colors = c(rgb(0, 0, 1, 0.5), rgb(0, 1, 1, 0.5))
variables <- c("gender", "family_class", "tech_access")
group <- c("private", "public")
 
# Create the matrix of the values.
Values <- matrix(c(sum(X_treat$gender)/nrow(X_treat), 
                   sum(X_treat$family_class)/nrow(X_treat), 
                   sum(X_treat$tech_access)/nrow(X_treat),
                   sum(X_control$gender)/nrow(X_control), 
                   sum(X_control$family_class)/nrow(X_control),
                   sum(X_control$tech_access)/nrow(X_control)),
                 nrow = 2, ncol = 3, byrow = TRUE)
 
# Create the bar chart
barplot(Values, main = "gender, family_class, tech_access", names.arg = variables,
                        xlab = "variables", ylab = "percentage",
                        col = colors, beside = TRUE)
 
# Add the legend to the chart
legend("topleft", group, cex = 0.7, fill = colors)
```

```{r}
num <- c(sum(group1_ind),
         sum(group2_ind),
         sum(group3_ind),
         sum(group4_ind),
         sum(group5_ind),
         sum(group6_ind),
         sum(group7_ind),
         sum(group8_ind))
label <- c("female, middle, tech", 
           "male, middle, tech", 
           "female, working, tech", 
           "male, working, tech", 
           "female, middle, no_tech", 
           "male, middle, no_tech", 
           "female, working, no_tech", 
           "male, working, no_tech")
par(mar = c(0.5, 0.5, 0.5, 0.5))
pie(num, label, radius=0.8)
```










